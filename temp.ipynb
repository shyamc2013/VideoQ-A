{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been successfully updated.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = '34_min.txt'\n",
    "\n",
    "# Read the contents of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace all occurrences of the pattern '(hh:mm) ' with a newline\n",
    "modified_content = re.sub(r'\\(\\d{2}:\\d{2}\\) ', '\\n', content)\n",
    "\n",
    "# Write the modified content back to the same file\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(modified_content)\n",
    "\n",
    "print(\"The file has been successfully updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader, TextLoader\n",
    "from langchain_community.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationChain, RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.chat_models.azure_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"b7d5aa82d15a4b99a1c730f681ec2bbc\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://hmh-digitalhub-azure-openai.openai.azure.com/\"\n",
    "os.environ[\"CHAT_MODEL\"] = \"gpt-35-turbo\"\n",
    "os.environ[\"CHAT_MODEL_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"text-embedding-ada-002\"\n",
    "os.environ[\"EMBEDDINGS_MODEL_DEPLOYMENT_NAME\"] = \"text-embedding-ada-002\"\n",
    "\n",
    "llm = AzureChatOpenAI(  \n",
    "            model_name =\"gpt-35-turbo\",\n",
    "            deployment_name= \"gpt-35-turbo\",\n",
    "            temperature=0,\n",
    "            openai_api_version =\"2023-07-01-preview\",\n",
    "            openai_api_key=\"b7d5aa82d15a4b99a1c730f681ec2bbc\",\n",
    "            openai_api_base=\"https://hmh-digitalhub-azure-openai.openai.azure.com/\"\n",
    "        )\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "loader = Docx2txtLoader(\"transcript/gen_ai.docx\")    \n",
    "documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "loader = TextLoader(\"34_min.txt\")    \n",
    "documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39740"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    }
   ],
   "source": [
    "max_para_len= 0\n",
    "for para in documents[0].page_content.split('\\n\\n'):\n",
    "    max_para_len= max(max_para_len, len(para))\n",
    "\n",
    "print(max_para_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size= 1000, chunk_overlap= 0)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919\n",
      "963\n",
      "963\n",
      "954\n",
      "959\n",
      "952\n",
      "953\n",
      "937\n",
      "960\n",
      "957\n",
      "946\n",
      "961\n",
      "990\n",
      "956\n",
      "970\n",
      "959\n",
      "918\n",
      "945\n",
      "961\n",
      "942\n",
      "919\n",
      "946\n",
      "971\n",
      "948\n",
      "970\n",
      "953\n",
      "671\n",
      "974\n",
      "968\n",
      "970\n",
      "671\n",
      "667\n",
      "950\n",
      "939\n",
      "948\n",
      "943\n",
      "926\n",
      "971\n",
      "949\n",
      "955\n",
      "977\n",
      "938\n",
      "466\n"
     ]
    }
   ],
   "source": [
    "for doc in chunked_documents:\n",
    "    print(len(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(chunked_documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query= 'Can I create good keyframes?'\n",
    "# query= 'What is attension mechanism?'\n",
    "# query= 'How is the audiobook getting created?'\n",
    "# query= 'Explain step by step how the feature film is getting created from the ebook.'\n",
    "# 12_min.txt\n",
    "# query= 'What are the key points for the better working of the model?'\n",
    "# query= 'How can I learn about different chain types?'\n",
    "# query= 'Why are we using custom prompts?'\n",
    "# query= 'What is the flow of the application from starting to end?'\n",
    "# query= 'Why do we not want our user to do prompt engineering to get the result in the format he wants? And are we helping the user to get the output in the format he wants?'\n",
    "# query= 'Who is selling the product and to whom? Also give the name of the product.'\n",
    "# query= 'How are we creating embeddings of the text?'\n",
    "# query= 'Explain step by step how are we setting up the chat prompt template? How are we taking the user input into it?'\n",
    "query= 'Who is Mike noop?'\n",
    "# query= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents= db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44228512\n",
      "we we sort of call the code right moment internally That's Mike noop co-founder and head of AI at zapier he sled the product team that helps over 10 million people automate workflows between 5 000 of your favorite applications many consider zapier to be the backbone of modern Automation in early 2022 Mike\n",
      "\n",
      "gave up his exact title to go all in on AI and zapier not just for product's sake but to figure out how to enable zapier's internal employees with AI I wanted to talk with Mike because he thinks about how to enable his employees with AI in ways that other Business Leaders don't gotten from the whole work\n",
      "\n",
      "and just said yeah we're gonna give everybody one week to go like step out of your day job and figure out how to use language models for yourself in this episode we talk about zapier's top internal use cases for AI that actually increase Revenue Mike's recommendations for how other businesses should get\n",
      "-----------------------------------------------------------------------------------------\n",
      "0.52233505\n",
      "to at least you know let's see if we can get a broad start on the problem by just like asking everybody to take some individual responsibility to go figure out how to use it um and that's that's paid off so like if I look down our top list I was actually just looking at some of the profiles of folks um so we've got like you know\n",
      "\n",
      "Chris our partner operations Specialist or Reed who's one of our AI product managers or Diane who's one of our Ops Specialists and PMs like those are 10 those tend to be the folks that have like figured out how to build these things that you know they tend to be in those operational roles where they're\n",
      "\n",
      "looking across a some sort of workflow that's matters to a department or have some good sense of like what the departmental goals are that they can deploy elements against and you had brought up enabling customers kind of like doing Common Sense training with the do's and don'ts of llms and what they're good at and what they're not\n",
      "-----------------------------------------------------------------------------------------\n",
      "0.5319265\n",
      "one week right now at the very beginning to get on the ground floor and go figure it out for yourself and you know take the take the individual initiative to go figure it out because it is going to be an important part of every future job that zapper and Beyond yeah no that's beautiful and Mike I just want to wrap\n",
      "\n",
      "it up with one last kind of fun question here um so there's a ton of AI news out there right there's stuff all over Twitter stuff all over the headlines what's the news that gets you excited what are you looking forward to seeing when it's coming out model architecture Innovation is uh when I'm like not spending time on\n",
      "\n",
      "useful AI use cases that's the like those are the papers I'm reading and like most interested in uh you know there's a side question of what can you predict about language models and AI at this point yeah and I think the things you can have a pretty strong degree of conviction around today are that latency is going to go down for token\n",
      "-----------------------------------------------------------------------------------------\n",
      "0.5360428\n",
      "started with internal Ai and his predictions on the future skill sets of knowledge workers and where zapier will be making headcount Investments to meet the a wave so Mike as I was researching for the interview one thing that stood out to me was how you've embraced different roles at zapier it seems like you filled in different\n",
      "\n",
      "gaps depending on what the business needed and last July you and your co-founder Brian both gave up your exact title and went all in on zapier at or AI at zapier what what does that mean what's that transition been like well uh probably I mean a personal story there plus the like the AI story I think\n",
      "\n",
      "there's two two different ones um you know on the personal side I think I've actually met quite a few um uh sort of third co-founders which is quite uh sort of got into zapper you know from the very early days it was always very clear from the very beginning that I think Wade was going to be CEO Brian was going to be CTO and so\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc[1])\n",
    "    print(doc[0].page_content)\n",
    "    print('-----------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= db.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= ChatPromptTemplate.from_template('''\n",
    "Answer the following question based only on the provided context. \n",
    "Do not respond with anything outside of the context. If you don't know, say, \"I don't know\"\n",
    "Think step by step before providing me detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain= create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain= create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RunnableParallel.invoke() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[273], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39m retrieval_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: query}, verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5090\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5091\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5092\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5093\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5095\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5096\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5097\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5098\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:495\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    492\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1784\u001b[0m         Output,\n\u001b[1;32m-> 1785\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1786\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m             config,\n\u001b[0;32m   1790\u001b[0m             run_manager,\n\u001b[0;32m   1791\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1792\u001b[0m         ),\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\config.py:427\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    426\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shyam\\.conda\\envs\\lc-prac\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:482\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    475\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    478\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m--> 482\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    484\u001b[0m             patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    486\u001b[0m         ),\n\u001b[0;32m    487\u001b[0m     }\n",
      "\u001b[1;31mTypeError\u001b[0m: RunnableParallel.invoke() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "response= retrieval_chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
